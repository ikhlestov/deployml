Here are the brief list of existed models speedup techniques with the links:

- Compression
  - Quantization
    - [Quantized Convolutional Neural Networks for Mobile Devices](https://arxiv.org/abs/1512.06473)
  - hashing
    - [Compressing Neural Networks with the Hashing Trick](https://arxiv.org/abs/1504.04788)
  - Pruning
    - [Learning both Weights and Connections for Efficient Neural Networks](https://arxiv.org/abs/1506.02626)
    - [Pruning Convolutional Neural Networks for Resource Efficient Inference](https://arxiv.org/abs/1611.06440)
  - Vector quantization
    - [Compressing Deep Convolutional Networks using Vector Quantization](https://arxiv.org/abs/1412.6115)
  - Huffman coding
    - [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/abs/1510.00149)
  - Factorizations
    - [Speeding up Convolutional Neural Networks with Low Rank Expansions](https://arxiv.org/abs/1405.3866)
    - [Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition](https://arxiv.org/abs/1412.6553)
    - [Ultimate tensorization: compressing convolutional and FC layers alike](https://arxiv.org/abs/1611.03214)
- Knowledge distillation
  - [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
  - [Sequence-Level Knowledge Distillation](https://arxiv.org/abs/1606.07947)
- low bit networks
  - [Training deep neural networks with low precision multiplications](https://arxiv.org/abs/1412.7024)
  - [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](https://arxiv.org/abs/1603.05279)
  - [Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations](https://arxiv.org/abs/1609.07061)
- I don't know exactly
  - [Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications](https://arxiv.org/abs/1511.06530)

Blog posts:

- [How to Quantize Neural Networks with TensorFlow](https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/)
- [Compression of neural networks](https://blog.themusio.com/2016/12/02/compression-of-neural-networks/)
- [Compressing and regularizing deep neural networks](https://www.oreilly.com/ideas/compressing-and-regularizing-deep-neural-networks)
- Deep Compression and EIE [slides](http://web.stanford.edu/class/ee380/Abstracts/160106-slides.pdf) and [video](http://on-demand.gputechconf.com/gtc/2016/video/S6561.html)
- [Pruning deep neural networks to make them fast and small(in pyTorch)](https://jacobgil.github.io/deeplearning/pruning-deep-learning)

